---
title: "Analyzing IMDb Ratings of Scooby-Doo Episodes"
author: "Regression Rockstars: James Cai, Steph Reinke, Sarah Wu, Michael Zhou"
date: "December 1, 2023"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load-pkg-data
#| warning: false
#| message: false

library(tidyverse)
library(tidymodels)
library(knitr) 
library(patchwork)
library(skimr)
library(broom)
library(dplyr)
library(parsnip)
library(workflowsets)

scoobydoo <- read_csv("data/Scooby-Doo Completed.csv")
```

# Introduction and Data:

## Introduction

Scooby-Doo is a popular animated TV show that follows a group of teenagers and a talking Great Dane, Scooby-Doo, as they solve mysteries involving supernatural monsters and creatures. Each episode typically involves seeking and scheming to find the villain, ending with a dramatic unmasking of the monster. The show focuses on themes of friendship and teamwork. The show originally aired on CBS from 1969 - 1976, but there has been many subseries and reboots since.

We are interested in researching Scoody-Doo IMDb ratings because we all enjoyed Scooby-Doo in our childhoods. We also think that finding certain predictors of animated TV series ratings is useful for the entertainment industry. Specifically, our findings could be useful to anyone looking to create an animated TV series and wanting to know what aspects make up a successful episode. In the paper, "Determining and Evaluating The Most Popular Cartoons Among Children Between 4 and 6 Years of Age" published in 2017, the authors criticize the use of violence, vulgar language, and horror music in Scooby-Doo ([BaÅŸal et. al. 2017](https://www.researchgate.net/publication/318108700_Determining_and_Evaluating_The_Most_Popular_Cartoons_Among_Children_Between_4_and_6_Years_of_Age)), yet we can't ignore the huge impact and popularity of Scooby-Doo. In 2013, Scooby-Doo was ranked the fifth greatest cartoon of all time ([TVGuide 2013](https://www.foxnews.com/entertainment/tv-guide-magazines-60-greatest-cartoons-of-all-time)). If Scooby-Doo continues to create spin-off shows, our findings about what makes a successful episode could inform their future episodes as well.

Our primary research question is what factors best explain the variability in the IMDb ratings of Scooby-Doo episodes? In other words, what elements tend to contribute to a successful episode? We want to investigate how various predictor variables in the dataset like `monster.amount`, `engagement`, character that unmasks the villain (`unmask.fred`, and such for all five characters part of the main group), `network`, and more, adequately explain the variability in IMDb ratings. We hypothesize that episodes with a higher monster count will have a better rating, since we think that there is more action and suspense in episodes with more monsters. We also hypothesize that the higher engagement, the worse the rating will be. This is because we think that people are more likely to write a review online when they dislike the episode rather than if they liked the episode. We think that episodes where Fred unmasked the villain will have a higher rating since he is the leader of the group and thus, we think that people will be more drawn to him. Finally, we think that episodes that aired on Cartoon Network will have a better rating, since we think that Cartoon Network has the ability to generate more positive responses since they specialize in cartoons and are pretty well-known. In our analysis, we would like to explore the interaction between these variables as well as consider other predictors in the dataset.

## Data

This Scooby-Doo data was found on the [TidyTuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-07-13/readme.md#scooby-doo-episodes) database on Github. The data originally comes from [Kaggle](https://www.kaggle.com/datasets/williamschooleman/scoobydoo-complete) and was manually aggregated by user [Plummye](https://www.kaggle.com/williamschooleman) in 2021. The curator took roughly one year to watch every Scooby-Doo iteration and track every variable in this dataset. It is noted that some of the values are subjective by nature of watching, but the original data data curator tried to keep the data collection consistent across the different episodes.

Each observation represents an episode from a rendition of the Scooby-Doo franchise up until February 25, 2021, including movies and specials. The variables that were measured include the series and episode name (which we will not use as predictor variables), network aired on, IMDb rating, engagement (represented by number of reviews on IMDb), and many details about what happened in each episode, such as how many monsters appeared, which character captured and unmasked the monster, the terrain of the episode, and more. There is a mix of both numerical and categorical characteristics.

The unmask variable is in the data as 6 separate columns with each column representing a person, such as `unmask.fred`, `unmask.velma`, etc. Before any of our analysis, we combined these columns into one singular column, `unmask_villain`. We also converted `imdb` from a `character` to a `double` as we want it to be a quantitative value.

Our response variable is `imdb`, while our predictor variables are `unmask_villain`, `monster.amount`, and `network`.

`imdb`: double, represents the rating on IMDb

`unmask_villain`: character, represents which character unmasked the villain (if any)

`monster.amount`: double, represents the number of monsters in the episode

`network`: character, represents the network the episode was aired on

```{r cleaning}
#| warning: false
scoobydoo <- mutate(scoobydoo, unmask_villain = 
                           ifelse(unmask.fred == TRUE, "Fred",
                           ifelse(unmask.daphnie == TRUE, "Daphne",
                           ifelse(unmask.velma == TRUE, "Velma",
                           ifelse(unmask.shaggy == TRUE, "Shaggy",
                           ifelse(unmask.scooby == TRUE, "Scooby", 
                           ifelse(unmask.other == TRUE, "Other", "None")))))))

scoobydoo <- scoobydoo |>
  mutate(imdb = as.numeric(imdb))
```

## Exploratory data analysis

```{r response-dist}
#| warning: false
#| message: false

e1 <- scoobydoo |>
  ggplot(aes(x = imdb)) +
  geom_histogram() +
  labs(x = "IMDb Rating",
       y = "Frequency",
       title = "IMDb Rating",
       caption = "Figure 1") +
  theme_minimal() +
    theme(plot.title = element_text(size=10))
```

```{r response-stats}
#| include = FALSE

scoobydoo |>
  skim(imdb) |>
  select(-skim_type, -skim_variable, -complete_rate,
         - numeric.hist) |> #remove these columns from output
  print(width = Inf) #print all columns
```

```{r qualpred-dist}
#| warning: false
#| message: false

e2 <- scoobydoo |>
  ggplot(aes(x = monster.amount)) +
  geom_histogram(bins=7) +
  labs(x = "Number of Monsters",
       y = "Frequency",
       title = "Number of Monsters",
       caption = "Figure 2") +
  theme_minimal() +
    theme(plot.title = element_text(size=10))
```

```{r qualpred-stats}
#| include = FALSE

scoobydoo |>
  skim(monster.amount) |>
  select(-skim_type, -skim_variable, -complete_rate,
         - numeric.hist) |> #remove these columns from output
  print(width = Inf) #print all columns
```

```{r patchwork, out.width = "75%"}
#| warning: false
#| message: false

e1 | e2
```

Figure 1: The distribution of our response variable IMDb ratings, `imdb`, is unimodal and roughly symmetrical. The mean is 7.278 and the standard deviation is 0.732. The minimum is 4.2 and the maximum is 9.6. There does not seem to be any significant outliers.

Figure 2: The distribution of the number of monsters, `monster.amount`, is unimodal and right skewed. The median is 1 monster and the IQR is 1 monster. The minimum is 0 monsters and the maximum is 19 monsters. There are a few episodes with notably high amounts of monsters, with 5 episodes having 13 or more monsters.

```{r unmask-dist}

e3 <- scoobydoo |>
  ggplot(aes(x = unmask_villain)) +
  geom_bar() +
  labs(x = "Network Aired On",
       y = "Frequency",
       title = "Distribution of Who Unmasked the Villain",
       caption = "Figure 3") +
  theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme(plot.title = element_text(size=10))
```

```{r catpred-dist}

e4 <- scoobydoo |>
  ggplot(aes(x = network)) +
  geom_bar() +
  labs(x = "Network Aired On",
       y = "Frequency",
       title = "Distribution of Network Aired On",
       caption = "Figure 4") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme(plot.title = element_text(size=10))
```

```{r showgraphs, out.width = "75%"}

e3 | e4
```

Figure 3: The distribution of who unmasks the villain, `unmask_villain`, shows that in a good majority of the episodes, no one unmasked the villain. However, out of the episodes where a villain was unmasked, Fred and Velma were the main characters that unmasked the villain.

Figure 4: The distribution of the network the episode aired on, `network`, shows that a good majority of the episodes aired on ABC. There were also considerable amounts of episodes that aired on Cartoon Network and Boomerang, while there are also networks that aired very few episodes, such as TBC and Adult Swim.

```{r qual}
#| warning: false

e5 <- scoobydoo |>
  ggplot(aes(x = as.factor(monster.amount), y = imdb)) +
  geom_boxplot() +
  labs(x = "Number of Monsters",
       y = "IMDb Rating",
       title = "Relationship between Number of Monsters and IMDb Rating",
       caption = "Figure 5") +
  theme_minimal() +
    theme(plot.title = element_text(size=10))
```

```{r correlation}
#| include = FALSE

scoobydoo |>
  na.omit(monster.amount) |>
  na.omit(imdb) |>
  summarise(r = cor(monster.amount, imdb)) |> pull()
```

```{r cat2}
#| warning: false
#| message: false

e6 <- scoobydoo |>
  ggplot(aes(x = unmask_villain, y = imdb)) +
  geom_boxplot() +
  labs(x = "Who Unmasks the Villain",
       y = "IMDB Rating",
       title = "Relationship between Who Unmasks the Villain and IMDB Rating",
       caption = "Figure 6"
       ) +
  theme_minimal() +
    theme(plot.title = element_text(size=10))
```

```{r plots, out.width = "75%"}
#| warning: false

e5/e6
```

Figure 5: From the distribution of the different boxplots for each number of total monsters included in `monster.amount`, we observe that many of the interquartile intervals of the boxplots overlap, meaning that their IMDB ratings are quite similar. However, there are still quite a few boxplots that do not overlap with a couple of the other boxplots, signaling that `monster.amount` may have some significant effects.

Figure 6: From the distribution of the different boxplots for each character included in `unmask_villain`, we observe that many of the interquartile intervals of the boxplots overlap, meaning that their IMDB ratings are quite similar. Since they all overlap, we should consider whether this variable is important for our model.

```{r cat}
#| warning: false
#| message: false

e7 <- scoobydoo |>
  ggplot(aes(x = network, y = imdb)) +
  geom_boxplot() +
  labs(x = "Network Aired On",
       y = "IMDb rating",
       title = "Relationship between Network Aired On and IMDb Rating",
       caption = "Figure 5"
       ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    theme(plot.title = element_text(size=10))
```

```{r interaction-eda}
#| warning: false
#| message: false

e8 <- scoobydoo |>
  filter(monster.amount < 8 & monster.amount > 0) |>
  ggplot(
    aes(x = as.factor(monster.amount), y = imdb, color = unmask_villain)) +
  geom_point() +
  labs(title = "Relationship between Number of Monsters and Unmask Villain",
       subtitle = "for 1-7 total monsters",
       x = "Number of Monsters",
       y = "IMDb Rating",
       color = "Unmasked by:",
       caption = "Figure 8") +
  theme_minimal() +
    theme(plot.title = element_text(size=10)) +
  theme(plot.subtitle = element_text(size=8))
```

```{r plots2}

e7
e8
```

Figure 7: From the distribution of the different boxplots for each network, we observe that many of the interquartile intervals of the boxplots overlap, meaning that their IMDb ratings are quite similar. It seems that Cartoon Network generally received the best ratings, while Warner Bros. Picture and The CW generally received the worst ratings. We also observe a few outliers in the distribution of IMDb ratings for some networks, such as Warner Home Video and ABC. Many of the networks have IMDb ratings that are pretty symmetrical, as the line representing the median is close to the middle of the box, such as in the case of CBS and The CW, but some are pretty skewed, such as in the case of Syndication, The WB, and Warner Bros. Picture.

Figure 8: Here, we explore the relationship between two of our predictor variables, `monster.amount` and `unmask_villain`. We are only showing a subset of the full relationship between the two variables, since as number of monsters increases, there is less and less data to observe a relationship. From this graph, we become interested in whether there is a significant relationship between these two variables on IMDb rating--for example, if there's one monster, is the effect on IMDb rating different for whether Fred unmasks the villain or if Velma unmasks the villain?

# Methodology:

Since our response variable is quantitative, we decided to use multiple linear regression for modeling. We also split our original data set into a training (75%) and testing (25%) set to attempt to prevent model overfitting. When choosing what predictor variables to use, there were many variables that were difficult to use due to the nature of the input, such as `monster.type`, where episodes with multiple monsters had all the types listed as one character string with a comma separating the different types. Other variables did not make sense to use, such as `title` and `culprit.name`, as we do not think variables like these would make good predictors. Other variables, such as `format` and `run.time` naturally had very little variability, and we thought it would be more interesting to create a model with variables that had more variability to be able to make better predictions. Finally, other variables that we were interested in, such as `motive` and `arrested`, had many NULL values, so we excluded them from our model as we wanted to have the most amount of data to work with. Therefore, taking all of these factors into consideration, the predictor variables we settled on were `network`, `monster.amount`, and `unmask_villain`. From our initial exploratory data analysis, we saw a lot of variation within the relationship between network and IMDB, so we wanted to include `network` as part of our model. The relationships between `monster.amount` and `unmask_villain` each with IMDB seemed less strong, but since we are interested in how these predictor variables affect IMDB as well as any interaction effects that could be made within the three variables, we included these two variables in our model as well. To compare our models, we plan on using 3-fold cross validation, and we also included a function to calculate adjusted R squared, AIC, and BIC, which we will use when choosing our final model.

For each model, we decided to create a recipe, where we performed these steps across all models:

1.  Simplified the number of networks in `networks` by using step_other with a threshold of 30.
2.  Created dummy variables for all nominal predictors using step_dummy.
3.  Removed predictors with zero variance using step_zv.

We tested a total of four different models. Since we knew that we wanted to include the variables, `network`, `monster.amount`, and `unmask_villain`, we tested different interactions between combinations of two of the three predictor variables.

Model 1: imdb \~ network + monster.amount + unmask_villain with no interaction terms

Model 2: imdb \~ network + monster.amount + unmask_villain with interaction between monster.amount and unmask_villain

Model 3: imdb \~ network + monster.amount + unmask_villain with interaction between monster.amount and network

Model 4: imdb \~ network + monster.amount + unmask_villain with interaction between unmask_villain and network

```{r function}
calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select (adj.r.squared, AIC, BIC)
}
```

```{r}
#| label: split

set.seed(6)
data_split <- initial_split(scoobydoo, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)

set.seed(6)
folds <- vfold_cv(train_data, v = 3)

model_spec <- linear_reg() |>
  set_engine("lm")
```

```{r}
#| label: recipe2

recipe_2 <- recipe(imdb ~ network + monster.amount + unmask_villain,
                    data = train_data) |>
  step_other(network, threshold = 30) |>
  step_interact(terms = ~ monster.amount:unmask_villain) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())

wflow2 <- workflow() |>
  add_model(model_spec) |>
  add_recipe(recipe_2)
```

```{r}
#| label: recipe3

recipe_3 <- recipe(imdb ~ network + monster.amount + unmask_villain,
                    data = train_data) |>
  step_other(network, threshold = 30) |>
  step_interact(terms = ~ monster.amount:network) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())

wflow3 <- workflow() |>
  add_model(model_spec) |>
  add_recipe(recipe_3)
```

```{r}
#| label: recipe1

recipe_1 <- recipe(imdb ~ network + monster.amount + unmask_villain,
                    data = train_data) |>
  step_other(network, threshold = 30) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())

wflow1 <- workflow() |>
  add_model(model_spec) |>
  add_recipe(recipe_1)
```

```{r}
#| label: recipe4

recipe_4 <- recipe(imdb ~ network + monster.amount + unmask_villain,
                    data = train_data) |>
  step_other(network, threshold = 30) |>
  step_interact(terms = ~ unmask_villain:network) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())

wflow4 <- workflow() |>
  add_model(model_spec) |>
  add_recipe(recipe_4)
```

```{r results='hide'}
#| label: fit

fit_rs1 <- wflow1 |>
  fit_resamples(resamples=folds,
                control = control_resamples(extract = calc_model_stats))

fit_rs2 <- wflow2 |>
  fit_resamples(resamples=folds,
                control = control_resamples(extract = calc_model_stats))

fit_rs3 <- wflow3 |>
  fit_resamples(resamples=folds,
                control = control_resamples(extract = calc_model_stats))

fit_rs4 <- wflow4 |>
  fit_resamples(resamples=folds,
                control = control_resamples(extract = calc_model_stats))

map_df (fit_rs1$.extracts, ~ .x[[1]] [[1]]) |>
  summarise (mean_adj_rsq = mean(adj.r.squared),
             mean_aic = mean(AIC),
             mean_bic = mean (BIC)) |>
  kable(digits = 3, caption = "wflow1")

map_df (fit_rs2$.extracts, ~ .x[[1]] [[1]]) |>
  summarise (mean_adj_rsq = mean(adj.r.squared),
             mean_aic = mean(AIC),
             mean_bic = mean (BIC)) |>
  kable(digits = 3, caption = "wflow2")

map_df (fit_rs3$.extracts, ~ .x[[1]] [[1]]) |>
  summarise (mean_adj_rsq = mean(adj.r.squared),
             mean_aic = mean(AIC),
             mean_bic = mean (BIC)) |>
  kable(digits = 3, caption = "wflow3")

map_df (fit_rs4$.extracts, ~ .x[[1]] [[1]]) |>
  summarise (mean_adj_rsq = mean(adj.r.squared),
             mean_aic = mean(AIC),
             mean_bic = mean (BIC)) |>
  kable(digits = 3, caption = "wflow4")
```

| Model \# | Mean Adjusted R-Squared | Mean AIC | Mean BIC |
|----------|-------------------------|----------|----------|
| 1        | 0.269                   | 610.982  | 676.177  |
| 2        | 0.333                   | 580.208  | 633.100  |
| 3        | 0.265                   | 606.73   | 649.782  |
| 4        | 0.303                   | 603.144  | 694.162  |

From these statistics, it is clear that Model #2 (imdb \~ network + monster.amount + unmask_villain with interaction between monster.amount and unmask_villain) had the highest mean adjusted R-squared and the lowest AIC and BIC values. Therefore, we will use this model as our final model.

# Results:

```{r finalmodel, out.width = "75%"}

scooby_fit <- wflow2 |>
  fit(data = train_data)

tidy(scooby_fit) |>
  kable(digits = 3)
```

```{r}

predictions <- predict(scooby_fit, new_data = train_data, type = "raw")
residuals <- train_data$imdb - predictions

scooby_aug <- data.frame(
  .fitted = predictions,
  .resid = residuals
) |>
  bind_cols(train_data)
```

```{r conditions}
#| warning: false

p1 <- ggplot(data = scooby_aug, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs. Predicted Values",
       x = "Predicted values", 
       y = "Residuals") +
  theme_minimal()

p2 <- ggplot(data = scooby_aug, aes(x = network, y = .resid)) +
  geom_point(alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Network", 
       x = "Network", 
       y = "Residuals") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

p3 <- ggplot(data = scooby_aug, aes(x = monster.amount, y = .resid)) +
  geom_point(alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Monster Amount",
       x = "Monster Amount", 
       y = "Residuals") +
  theme_minimal()

p4 <- ggplot(data = scooby_aug, aes(x = unmask_villain, y = .resid)) +
  geom_point(alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Unmask Villain", 
       x = "Unmasked By",
       y = "Residuals") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

```{r, out.width = "75%"}

(p1)

(p2 | p3 | p4)
```

```{r normality}
#| warning: false
#| message: false

p5 <- scooby_aug |>
  ggplot(aes(x = .resid)) +
  geom_histogram() +
  labs(title = "Distribution of Residuals",
       x = "Residuals",
       y = "Frequency") +
  theme_minimal()
```

```{r independence, out.width = "75%"}
#| warning: false

p6 <- ggplot(scooby_aug, aes(y = .resid, x = 1:nrow(scooby_aug))) +
  geom_line() +
  geom_point(size = 0.05) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals Over Time",
       x = "Order of Episodes",
       y = "Residuals") +
  theme_minimal()

(p5 | p6)
```

```{r predictions}

train_pred <- predict(scooby_fit, train_data) |>
  bind_cols(train_data)

test_pred <- predict(scooby_fit, test_data) |>
  bind_cols(test_data)
```

```{r rsqrmse}

rsq_train <- rsq(train_pred, truth = imdb, estimate = .pred)
rsq_test <- rsq(test_pred, truth = imdb, estimate = .pred)

rmse_train <- rmse(train_pred, truth = imdb, estimate = .pred)
rmse_test <- rmse(test_pred, truth = imdb, estimate = .pred)
```

This is the model output for our final model, where we predicted `imdb` with `network`+ `monster.amount` + `unmask_villain` with an interaction between `monster.amount` and `unmask_villain`. From the p-values of the coefficients, we see that 8 of 18 predictor variables have significant values when using a threshold of 0.05. Most of the low p-values in coefficients come from `network`. It is interesting that `monster.amount_x_unmask_villainVelma` was significant while none of the other interactions were. Another significant coefficient value that is worthy to note is `unmask_villain_None`. From our earlier EDA, we did see that 'None' was the largest category for `unmask_villain`, so it is possible the coefficient value is significant only because there was a large amount of data for it.

| Dataset  | R-Squared | RMSE  |
|----------|-----------|-------|
| Training | 0.483     | 0.554 |
| Testing  | 0.506     | 0.423 |

The model's R-squared value on the training data is approximately 0.483, which means that approximately 48.3 percent of the variation in the response variable explained by our regression model. However, the model's R-squared value on the testing data is approximately 0.506, which is actually surprisingly higher than the value for our training data. Since the R-squared values are relatively close, we are not too concerned about our model having overfit the data. The same trend is observed in our RMSE values, as the RMSE for the training data is 0.554, while it is 0.423 for the testing data.

All in all, from our model, we see that `network` and `monster.amount` seem to be significant when predicting IMDB ratings for different Scooby-Doo episodes. We observe that the variable, `unmask_villain`, as well as the interaction of it with `monster.amount` carry very few significant coefficients, so we may reconsider including this variable as a predictor variable in future models. However, for this project, since we were especially interested in using `unmask_villain` as a predictor variable, we retain it in our final model.
